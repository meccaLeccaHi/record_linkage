{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Display the output of plotting commands inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import python modules\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# Load 'helper utilities'\n",
    "from linkage_tools import *\n",
    "l = Linker()\n",
    "\n",
    "# Load classifier objects\n",
    "from Probabilistic import *\n",
    "from Perceptron import *\n",
    "from NeuralNetwork import *\n",
    "\n",
    "#import warnings\n",
    "#warnings.filterwarnings('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set print output toggle\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define linking variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define tables\n",
    "tableA = '''patient_discharges JOIN newborns\n",
    "ON patient_discharges.id = newborns.patient_discharge_id\n",
    "LEFT OUTER JOIN patient_payers AS newborn_payers\n",
    "ON newborn_payers.patient_discharge_id = patient_discharges.id'''\n",
    "\n",
    "tableB = '''births JOIN deliveries \n",
    "ON births.delivery_id = deliveries.id\n",
    "JOIN patient_discharges AS delivery_discharges\n",
    "ON deliveries.patient_discharge_id = delivery_discharges.id\n",
    "LEFT OUTER JOIN patient_payers AS delivery_payers\n",
    "ON delivery_payers.patient_discharge_id = delivery_discharges.id'''\n",
    "\n",
    "# Define fields to compare tables by\n",
    "tableA_fields = ['patient_discharges.sex_id','newborns.id','newborns.cesarean_section', \n",
    "            'patient_discharges.zip_code','patient_discharges.race_id','patient_discharges.ethnicity_id', \n",
    "                'newborn_payers.payer_type_of_coverage_id','patient_discharges.principal_language_spoken', \n",
    "                 'patient_discharges.discharged_on','newborn_payers.payer_category_id',\n",
    "                 'newborn_payers.plan_code_number','newborns.birth_weight_group_id',\n",
    "                 'newborns.gestational_age_group_id','newborns.plurality_group_id']\n",
    "\n",
    "tableB_fields = ['births.sex_id','births.newborn_id','births.delivery_route_id', \n",
    "            'births.mothers_residence_zip_code','delivery_discharges.race_id','delivery_discharges.ethnicity_id', \n",
    "                'delivery_payers.payer_type_of_coverage_id','delivery_discharges.principal_language_spoken', \n",
    "                 'delivery_discharges.discharged_on','delivery_payers.payer_category_id',\n",
    "                 'delivery_payers.plan_code_number','births.birth_weight',\n",
    "                 'births.gestational_age_ob_estimate','births.plurality'] \n",
    "\n",
    "# Define new names for each comparison\n",
    "simple_feature_names = ['sex_id','c_section','zip_code','race','ethnicity','payer_type_of_coverage_id',\n",
    "                'language_spoken','discharge','payer_category_id','plan_code_number','plurality']\n",
    "\n",
    "complex_feature_names = ['weight_group_id','gest_age_group_id']\n",
    "\n",
    "simple_feature_num = 11\n",
    "\n",
    "# Exact matching only\n",
    "simple_features = [val for pair in zip([x+'_match' for x in simple_feature_names],\n",
    "                                    [x+'_nomatch' for x in simple_feature_names]) for val in pair]\n",
    "\n",
    "# Including non-exact matching\n",
    "complex_features = [val for pair in zip([x+'_match' for x in complex_feature_names],\n",
    "                     [x+'_fuzzymatch' for x in complex_feature_names],\n",
    "                     [x+'_nomatch' for x in complex_feature_names]) for val in pair]\n",
    "\n",
    "feature_vals = simple_features + complex_features\n",
    "\n",
    "# Define bins to use for certain fields\n",
    "age_bins = np.concatenate(([0,1,24], np.arange(25, 36, 2),[99]), axis=0)\n",
    "bw_bins = np.concatenate(([1], np.arange(500, 2001, 250),[2500,9999]), axis=0)\n",
    "\n",
    "cesarean_ids = [1, 11, 21, 31, 2, 12, 22, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_condition_vals(cross_table):\n",
    "    condition_vals = [  (cross_table['sex_id_x'],cross_table['sex_id_y']),\n",
    "                (cross_table['cesarean_section'],\n",
    "                 pd.Series([x in cesarean_ids for x in cross_table['delivery_route_id']])),\n",
    "                (cross_table['zip_code'],cross_table['mothers_residence_zip_code']),\n",
    "                (cross_table['race_id_x'],cross_table['race_id_y']),\n",
    "                (cross_table['ethnicity_id_x'],cross_table['ethnicity_id_y']),\n",
    "                (cross_table['payer_type_of_coverage_id_x'],cross_table['payer_type_of_coverage_id_y']),\n",
    "                ((cross_table['principal_language_spoken_x'].str[:3]).str.lower(),\n",
    "                 (cross_table['principal_language_spoken_y'].str[:3]).str.lower()),\n",
    "                (cross_table['discharged_on_x'],cross_table['discharged_on_y']),\n",
    "                (cross_table['payer_category_id_x'],cross_table['payer_category_id_y']),\n",
    "                (cross_table['plan_code_number_x'],cross_table['plan_code_number_y']),\n",
    "                ((cross_table['plurality_group_id']==1),(cross_table['plurality']==1)),\n",
    "                (cross_table['birth_weight_group_id'],\n",
    "                     l.digitize_series(cross_table['birth_weight'],bw_bins)),\n",
    "                (cross_table['gestational_age_group_id'],\n",
    "                     l.digitize_series(cross_table['gestational_age_ob_estimate']/7,age_bins))  ] \n",
    "    return(condition_vals)\n",
    "\n",
    "def getBools(cross_table):\n",
    "    \n",
    "    def bool_test(condition_val_pair):\n",
    "        # Exact matching of fields\n",
    "        condA,condB = condition_val_pair\n",
    "        #condA,condB = pd.to_numeric(condA),pd.to_numeric(condB)\n",
    "        fields_notnull = np.logical_and(condA.notnull(),condB.notnull())\n",
    "        return(np.vstack([np.array(condA.values==condB.values)&fields_notnull,\n",
    "                          np.array(condA.values!=condB.values)&fields_notnull]))\n",
    "    \n",
    "    def bool_test_complex(condition_val_pair):\n",
    "        # 'Fuzzy' matching of fields\n",
    "        condA,condB = condition_val_pair\n",
    "        fields_notnull = np.logical_and(condA.notnull(),condB.notnull())\n",
    "        condA,condB = pd.to_numeric(condA),pd.to_numeric(condB)\n",
    "        condA,condB = np.nan_to_num(condA),np.nan_to_num(condB)\n",
    "        return(np.vstack([np.squeeze(np.diff(zip(condA,condB))==0)&fields_notnull,\n",
    "                          np.squeeze(np.diff(zip(condA,condB))==1)&fields_notnull,\n",
    "                          np.squeeze(np.diff(zip(condA,condB))>1)&fields_notnull]))\n",
    "\n",
    "    condition_vals = get_condition_vals(cross_table)\n",
    "    simple_bools = [bool_test(x) for x in condition_vals[0:simple_feature_num]] # Simple features\n",
    "    complex_bools = [bool_test_complex(x) for x in condition_vals[simple_feature_num:]] # Complex features\n",
    "\n",
    "    return(np.concatenate(simple_bools+complex_bools).T)\n",
    "\n",
    "#def getMissing(cross_table):\n",
    "#    def none_test(condition_val_pair):\n",
    "#        condA,condB = condition_val_pair\n",
    "#        aNones = [x is None for x in condA]\n",
    "#        bNones = [x is None for x in condB]\n",
    "#        return(np.vstack([np.logical_or(aNones,bNones),np.logical_or(aNones,bNones)]))\n",
    "#   condition_vals = get_condition_vals(cross_table)\n",
    "#    return(np.concatenate([none_test(x) for x in condition_vals]).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find all possible values for 1st blocking field ('hospital_id')\n",
    "stmt = '''\n",
    "SELECT DISTINCT patient_discharges.hospital_id\n",
    "FROM patient_discharges \n",
    "UNION\n",
    "SELECT DISTINCT births.hospital_id\n",
    "FROM births;'''\n",
    "block_list1 = [x[0] for x in l.exec_sql(stmt).values.tolist()]\n",
    "\n",
    "block_list1 = block_list1[0:3]\n",
    "\n",
    "# Find all possible values for 2nd blocking field ('date_of_delivery')\n",
    "stmt = '''\n",
    "SELECT DISTINCT births.date_of_delivery\n",
    "FROM births;'''\n",
    "block_list2 = [x[0] for x in l.exec_sql(stmt).values.tolist()]\n",
    "\n",
    "block_list2 = block_list2[500:650]\n",
    "\n",
    "block_prod = [(str(x),str(y)) for x in block_list1 for y in block_list2]\n",
    "\n",
    "# Assign strings to select each block\n",
    "blocking_stmt1 = '''\n",
    "        SELECT patient_discharges.id AS pdd_id, newborns.id AS newb_id,%s \n",
    "        FROM %s\n",
    "        WHERE patient_discharges.hospital_id = %r\n",
    "        AND patient_discharges.date_of_birth = %r;\n",
    "        '''\n",
    "blocking_stmt2 = '''\n",
    "        SELECT births.id AS bc_id,%s\n",
    "        FROM %s\n",
    "        WHERE births.hospital_id = %r\n",
    "        AND births.date_of_delivery = %r;\n",
    "        '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create classifiers and iterate over each block\n",
    "- Create [probabilistic] and [perceptron] classifier (train each on selected subset of features ['simple' vs. 'complex'])\n",
    "- Loop over each block and convert to pandas dataframe\n",
    "- Cross join the features-of-interest from both blocks\n",
    "- Apply conditional statements to each column of dataframe\n",
    "- Return match score from each learner\n",
    "- Maximize pairing with Kuhn-Munkres (i.e. Hungarian) Algorithm [more info]\n",
    "- Compare guesses to truth, update weights/probabilities\n",
    "\n",
    "[more info]: https://pypi.python.org/pypi/munkres/\n",
    "[perceptron]: http://glowingpython.blogspot.com/2011/10/perceptron.html\n",
    "[probabilistic]: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5005943/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1, blockA: 21 records, blockB: 21 records  \n",
      "Average block duration: 0.42 seconds\n",
      "'Quality': [103.75, 86.53, 87.02, 74.63, 74.43]\n",
      "% correct: [90.8575, 91.1743, 95.0491, 90.6698, 90.4785]\n",
      "Average epoch duration: 4.386 minutes\n",
      "Epoch #2, blockA: 21 records, blockB: 21 records  \n",
      "Average block duration: 0.37 seconds\n",
      "'Quality': [157.46, 157.26, 153.99, 154.23, 154.99]\n",
      "% correct: [99.8709, 99.8862, 99.7876, 99.8627, 99.8768]\n",
      "Average epoch duration: 4.073 minutes\n",
      "Epoch #3, blockA: 20 records, blockB: 20 records  "
     ]
    }
   ],
   "source": [
    "learning_rate_pt = 0.01 # small, constant learning rate for perceptron\n",
    "learning_rate_nn = 0.1\n",
    "\n",
    "# Create instances of classifiers\n",
    "classifiers = [Probabilistic(simple_features),\n",
    "                Probabilistic(feature_vals),\n",
    "                Perceptron(simple_features,learningrate = learning_rate_pt),\n",
    "                Perceptron(feature_vals,learningrate = learning_rate_pt),\n",
    "                NeuralNetwork(feature_vals,10,1,learningrate = learning_rate_nn)]\n",
    "\n",
    "classifier_names = ['pr_simple','pr_complex','pt_simple','pt_complex','nn_complex']\n",
    "\n",
    "#classifier_features = [simple_features,feature_vals,simple_features,feature_vals,feature_vals]\n",
    "class_prog = [True] * len(classifiers)\n",
    "\n",
    "iter_qual_list = []\n",
    "iter_score_list = []\n",
    "block_duration = []\n",
    "epoch_duration = []\n",
    "epoch_count = 0\n",
    "verb_str = '\\rEpoch #{}, blockA: {} records, blockB: {} records  ' # verbose output\n",
    "\n",
    "while True:\n",
    "    epoch_start = time.time() # Measure epoch duration\n",
    "    big_bool = pd.DataFrame(columns=feature_vals)  \n",
    "\n",
    "    for block1, block2 in block_prod:\n",
    "        \n",
    "        block_start = time.time() # Measure block duration\n",
    "\n",
    "        # Create blocks\n",
    "        blockA = l.exec_sql(blocking_stmt1 % (\",\".join(tableA_fields),tableA,block1,block2))\n",
    "        blockB = l.exec_sql(blocking_stmt2 % (\",\".join(tableB_fields),tableB,block1,block2))\n",
    "        \n",
    "        if verbose:\n",
    "            sys.stdout.write(verb_str.format(epoch_count+1, len(blockA), len(blockB)))\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "        # Check that neither block is empty - If empty, skip to next record-pair in loop\n",
    "        if (len(blockA)==0) or (len(blockB)==0):\n",
    "            continue\n",
    "\n",
    "        # Cross-join both blocks\n",
    "        cross_table = l.df_crossjoin(blockA, blockB)\n",
    "\n",
    "        # Count field matches and dump each into new column of dataframe\n",
    "        bool_table = pd.DataFrame.from_items(zip(feature_vals,getBools(cross_table).T))\n",
    "        # bool_table = pd.DataFrame.from_items(zip(feature_vals,[eval(x).values for x in conditions]))\n",
    "\n",
    "        # Add record-id columns to boolean table\n",
    "        bool_table['newb_id'] = cross_table['newb_id'].values # Actual newborn id\n",
    "        bool_table['bc_id'] = cross_table['bc_id'].values  # Actual bc id\n",
    "        bool_table['prev_newb_id'] = cross_table['newborn_id'].values # Originally matched newborn id \n",
    "        \n",
    "        # Compare with previously-linked newborn id\n",
    "        bool_table['real_match'] = (cross_table['newborn_id']==cross_table['newb_id']).values \n",
    "        for x in classifier_names:\n",
    "            bool_table['match_'+x] = False # Create new columns for later\n",
    "        \n",
    "        # STILL NEED TO RE-DISTRIBUTE THE WEIGHTS FOR MISSING VALUES-\n",
    "        # Get guesses from classifiers\n",
    "        record_id = bool_table[['newb_id','bc_id']]\n",
    "        for x,y in zip(classifiers,classifier_names):\n",
    "            link_inds,link_score = x.query(bool_table[x.features],record_id) \n",
    "            bool_table['lscore_'+y] = link_score\n",
    "            bool_table.loc[link_inds,'match_'+y] = True # Label nominees as such  \n",
    "        \n",
    "        # Aggregate field-match tables (Booleans) within loop\n",
    "        big_bool = pd.concat([big_bool,bool_table])\n",
    "    \n",
    "        # Time duration of each block\n",
    "        block_duration.append(time.time()-block_start)\n",
    "        \n",
    "    # Check progress of each classifier \n",
    "    classifiers_masked = [elem for elem,z in zip(classifiers,class_prog) if z==True]\n",
    "    classifier_names_masked = [elem for elem,z in zip(classifier_names,class_prog) if z==True]\n",
    "    \n",
    "    # Train classifiers (if still progressing)\n",
    "    train_classifier = lambda x,y: x.train(big_bool[x.features],big_bool['real_match'],big_bool['match_'+y])\n",
    "    map(train_classifier,classifiers,classifier_names)\n",
    "    \n",
    "    # Measure accuracy of each classifier\n",
    "    score_list = [((big_bool['match_'+x]==big_bool['real_match'])*1).tolist() for x in classifier_names]\n",
    "    cur_scores = [sum(x)*100.0/len(x) for x in score_list]\n",
    "    iter_score_list.append(cur_scores)\n",
    "    \n",
    "    # Set range of linkage scores (0 through 100 percentiles) i.e. decision threshold (theta)\n",
    "    set_thetas = range(0,100,10)\n",
    "    \n",
    "    # Update precision & recall of classifiers\n",
    "    for x,y in zip(classifiers,classifier_names):\n",
    "        x.prec_recall(big_bool['lscore_'+y],set_thetas,big_bool['real_match'])\n",
    "    \n",
    "    # Compare current iteration 'quality' with previous iterations\n",
    "    cur_iter_qual = [x.iter_qual_list[-1] for x in classifiers]\n",
    "    prev_iter_qual = [x.iter_qual_list[-2] for x in classifiers]\n",
    "    class_prog = np.subtract(cur_iter_qual,prev_iter_qual)>.1\n",
    "    iter_qual_list.append(cur_iter_qual)\n",
    "    \n",
    "    # If not improving > Break from outer loop\n",
    "    if not any(class_prog):\n",
    "        print str.format('\\n'+'-'*80+\"\\n'Quality' achieved! @ {}\",[ float('%.2f' % x) for x in cur_iter_qual ])        \n",
    "        break\n",
    "\n",
    "    # Time duration of each iteration\n",
    "    epoch_duration.append(time.time()-epoch_start)\n",
    "    \n",
    "    epoch_count += 1 # Increment epoch counter\n",
    "    \n",
    "    if verbose:\n",
    "        print str.format('\\nAverage block duration: {0:.2f} seconds', np.mean(block_duration))\n",
    "        print str.format(\"'Quality': {}\", [ float('%.2f' % x) for x in cur_iter_qual ])\n",
    "        print str.format('% correct: {}', [ float('%.4f' % x) for x in cur_scores ])\n",
    "        print str.format('Average epoch duration: {0:.3f} minutes', np.mean(epoch_duration)/60.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot precision/recall for each epoch of probabilistic record linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prec_list = [o.precision_list for o in classifiers]\n",
    "plot_recall_list = [o.recall_list for o in classifiers]\n",
    "color_list = ['b','g','r','c','m'][slice(len(classifiers))]\n",
    "\n",
    "ncol = int(np.ceil(np.sqrt(len(plot_prec_list[0]))))\n",
    "nrow = int(np.ceil(len(plot_recall_list[0])/float(ncol)))\n",
    "fig, axs = plt.subplots(nrow, ncol, sharex='col', sharey='row')\n",
    "for i, ax in enumerate(fig.axes[0:len(plot_prec_list[0])]):   \n",
    "    plot_curves = lambda w,x,y: ax.plot(w[i], x[i], '-', linewidth=2, marker='o', color=y)\n",
    "    map(plot_curves,plot_prec_list,plot_recall_list,color_list)\n",
    "    ax.set_xlim([0,110])\n",
    "    ax.set_ylim([0,110])\n",
    "    ax.set_title(\"iter. # {}\".format(str(i)), fontsize=12)\n",
    "    ax.set(aspect='equal')    \n",
    "else:\n",
    "    ax.set_xlabel('precision', fontsize=12)\n",
    "    ax.set_ylabel('recall', fontsize=12)\n",
    "\n",
    "fig.set_size_inches(4.875*ncol,4.5*nrow)\n",
    "plt.show()\n",
    "fig.savefig('prec_recall.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot accuracy for each classifier over all epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(1,epoch_count+2)\n",
    "y = 100-np.array(iter_score_list, ndmin=2, dtype=np.float).T\n",
    "fig = plt.figure()\n",
    "plt.yscale('log')\n",
    "plt.ylim((y.min(),100))\n",
    "plt.title(\"classifier accuracy\")\n",
    "for quals,cl,col in zip(y,classifier_names,color_list):\n",
    "    plt.plot(x,quals,label = cl+' [min:'+str('%.2f' % quals.min())+'%]',color=col)\n",
    "plt.xlabel(\"epoch count\")\n",
    "plt.ylabel(\"% incorrect\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('classifier_accuracy.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show table of weights/probabilities for each classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"\"><tr style=\"\"><td style=\"\">classifier</td><td style=\"\">sex_id_match</td><td style=\"\">sex_id_nomatch</td><td style=\"\">c_section_match</td><td style=\"\">c_section_nomatch</td><td style=\"\">zip_code_match</td><td style=\"\">zip_code_nomatch</td><td style=\"\">race_match</td><td style=\"\">race_nomatch</td><td style=\"\">ethnicity_match</td><td style=\"\">ethnicity_nomatch</td><td style=\"\">payer_type_of_coverage_id_match</td><td style=\"\">payer_type_of_coverage_id_nomatch</td><td style=\"\">language_spoken_match</td><td style=\"\">language_spoken_nomatch</td><td style=\"\">discharge_match</td><td style=\"\">discharge_nomatch</td><td style=\"\">payer_category_id_match</td><td style=\"\">payer_category_id_nomatch</td><td style=\"\">plan_code_number_match</td><td style=\"\">plan_code_number_nomatch</td><td style=\"\">plurality_match</td><td style=\"\">plurality_nomatch</td><td style=\"\">weight_group_id_match</td><td style=\"\">weight_group_id_fuzzymatch</td><td style=\"\">weight_group_id_nomatch</td><td style=\"\">gest_age_group_id_match</td><td style=\"\">gest_age_group_id_fuzzymatch</td><td style=\"\">gest_age_group_id_nomatch</td><td style=\"\">bias</td></tr><tr style=\"\"><td style=\"\">prob_simple</td><td style=\"\">0.981024667932</td><td style=\"\">0.0185009487666</td><td style=\"\">0.992409867173</td><td style=\"\">0.00759013282732</td><td style=\"\">0.895161290323</td><td style=\"\">0.103889943074</td><td style=\"\">0.890891840607</td><td style=\"\">0.102466793169</td><td style=\"\">0.933586337761</td><td style=\"\">0.0645161290323</td><td style=\"\">0.0</td><td style=\"\">0.0</td><td style=\"\">0.916034155598</td><td style=\"\">0.0787476280835</td><td style=\"\">0.890417457306</td><td style=\"\">0.109582542694</td><td style=\"\">0.0</td><td style=\"\">0.0</td><td style=\"\">0.0</td><td style=\"\">0.0</td><td style=\"\">0.996204933586</td><td style=\"\">0.00379506641366</td><td style=\"\">nan</td><td style=\"\">nan</td><td style=\"\">nan</td><td style=\"\">nan</td><td style=\"\">nan</td><td style=\"\">nan</td><td style=\"\">nan</td></tr><tr style=\"\"><td style=\"\">prob_complex</td><td style=\"\">0.981024667932</td><td style=\"\">0.0185009487666</td><td style=\"\">0.992409867173</td><td style=\"\">0.00759013282732</td><td style=\"\">0.895161290323</td><td style=\"\">0.103889943074</td><td style=\"\">0.890891840607</td><td style=\"\">0.102466793169</td><td style=\"\">0.933586337761</td><td style=\"\">0.0645161290323</td><td style=\"\">0.0</td><td style=\"\">0.0</td><td style=\"\">0.916034155598</td><td style=\"\">0.0787476280835</td><td style=\"\">0.890417457306</td><td style=\"\">0.109582542694</td><td style=\"\">0.0</td><td style=\"\">0.0</td><td style=\"\">0.0</td><td style=\"\">0.0</td><td style=\"\">0.996204933586</td><td style=\"\">0.00379506641366</td><td style=\"\">0.076375711575</td><td style=\"\">0.00569259962049</td><td style=\"\">0.000474383301708</td><td style=\"\">0.0445920303605</td><td style=\"\">0.0716318785579</td><td style=\"\">0.00189753320683</td><td style=\"\">nan</td></tr><tr style=\"\"><td style=\"\">perc_simple</td><td style=\"\">14.9756052578</td><td style=\"\">-14.9821836876</td><td style=\"\">0.165228567641</td><td style=\"\">-0.165705402598</td><td style=\"\">16.9466069364</td><td style=\"\">-16.9343576373</td><td style=\"\">15.8023694142</td><td style=\"\">-15.8127165231</td><td style=\"\">12.6697030038</td><td style=\"\">-12.6692080655</td><td style=\"\">0.00902179801272</td><td style=\"\">0.00555401178594</td><td style=\"\">3.33025667212</td><td style=\"\">-3.33002813906</td><td style=\"\">2.00377015014</td><td style=\"\">-2.01972945706</td><td style=\"\">0.00178702274153</td><td style=\"\">-0.00532458834501</td><td style=\"\">0.000680941094566</td><td style=\"\">0.00224023011689</td><td style=\"\">1.95375899456</td><td style=\"\">-1.95284294058</td><td style=\"\">nan</td><td style=\"\">nan</td><td style=\"\">nan</td><td style=\"\">nan</td><td style=\"\">nan</td><td style=\"\">nan</td><td style=\"\">0.00894549382414</td></tr><tr style=\"\"><td style=\"\">perc_complex</td><td style=\"\">0.625412755749</td><td style=\"\">-0.617133943728</td><td style=\"\">1.26053665737</td><td style=\"\">-1.25903667607</td><td style=\"\">1.69771383109</td><td style=\"\">-1.68794997676</td><td style=\"\">0.238962895236</td><td style=\"\">-0.247801759969</td><td style=\"\">0.114620767972</td><td style=\"\">-0.112326890011</td><td style=\"\">-0.00770795103945</td><td style=\"\">0.00970571567555</td><td style=\"\">0.231849030812</td><td style=\"\">-0.230658364141</td><td style=\"\">0.807780056103</td><td style=\"\">-0.796405555595</td><td style=\"\">-0.00487802246161</td><td style=\"\">-0.0040530087758</td><td style=\"\">-0.000670760572659</td><td style=\"\">-0.00100497836549</td><td style=\"\">1.91322063187</td><td style=\"\">-1.91371700726</td><td style=\"\">0.563531152275</td><td style=\"\">-0.189731123446</td><td style=\"\">-0.29415407207</td><td style=\"\">-0.00994070815628</td><td style=\"\">0.420105650789</td><td style=\"\">-0.357070636284</td><td style=\"\">0.00102084488835</td></tr></table>"
      ],
      "text/plain": [
       "<itable.itable.PrettyTable at 0x7ff1362cccd0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itable\n",
    "\n",
    "# Assemble table elements into dataframe\n",
    "pt_simple_weights = classifiers[2].weights.tolist()[:-1] + \\\n",
    "    [None]*len(complex_features) + [classifiers[2].weights.tolist()[-1]]\n",
    "\n",
    "weight_list = [['prob_simple'] + classifiers[0].m_probs.tolist() + [None],\n",
    "               ['prob_complex'] + classifiers[1].m_probs.tolist() + [None],\n",
    "               ['perc_simple'] + pt_simple_weights,\n",
    "               ['perc_complex'] + classifiers[3].weights.tolist()]\n",
    "df = pd.DataFrame(weight_list, columns=['classifier']+feature_vals+['bias'])\n",
    "\n",
    "# Save table as html file\n",
    "my_file = open('weights.html', 'w')\n",
    "my_file.write(df.to_html())\n",
    "#pt_simple_weights = classifiers[2].weights.tolist()\n",
    "my_file.close()\n",
    "\n",
    "# Show table as ipython figure\n",
    "itable.PrettyTable(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organize and export 'wrong answers' to spreadsheet (.csv) files [1 spreadsheet/classifier]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot concatenate 'str' and 'instance' objects",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-65972798a369>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Gather incorrect matches for each classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mscore_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbig_bool\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'match_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mbig_bool\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'real_match'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclassifiers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot concatenate 'str' and 'instance' objects"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Select features to export\n",
    "feature_list = ['bc_id','real_match','newb_id']+feature_vals\n",
    "\n",
    "# Gather incorrect matches for each classifier\n",
    "score_list = [((big_bool['match_'+x]==big_bool['real_match'])*1).tolist() for x in classifiers]\n",
    "\n",
    "for sc,cl in zip(score_list,classifiers):\n",
    "    errors = big_bool[feature_list].iloc[[x==0 for x in sc],:].sort(['bc_id','real_match'], ascending=[1,0])\n",
    "    errors.to_csv('linkage_error_'+cl+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
