{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Import python modules\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import itable\n",
    "\n",
    "from linkage_tools import *\n",
    "from Probabilistic import *\n",
    "from Perceptron import *\n",
    "\n",
    "from NeuralNetwork import *\n",
    "\n",
    "l = Linker()\n",
    "\n",
    "#import warnings\n",
    "#warnings.filterwarnings('error')\n",
    "\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define linking variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tables\n",
    "tableA = '''patient_discharges JOIN newborns\n",
    "ON patient_discharges.id = newborns.patient_discharge_id\n",
    "LEFT OUTER JOIN patient_payers AS newborn_payers\n",
    "ON newborn_payers.patient_discharge_id = patient_discharges.id'''\n",
    "\n",
    "tableB = '''births JOIN deliveries \n",
    "ON births.delivery_id = deliveries.id\n",
    "JOIN patient_discharges AS delivery_discharges\n",
    "ON deliveries.patient_discharge_id = delivery_discharges.id\n",
    "LEFT OUTER JOIN patient_payers AS delivery_payers\n",
    "ON delivery_payers.patient_discharge_id = delivery_discharges.id'''\n",
    "\n",
    "# Define fields to compare tables by\n",
    "tableA_fields = ['patient_discharges.sex_id','newborns.id','newborns.cesarean_section', \n",
    "            'patient_discharges.zip_code','patient_discharges.race_id','patient_discharges.ethnicity_id', \n",
    "                'newborn_payers.payer_type_of_coverage_id','patient_discharges.principal_language_spoken', \n",
    "                 'patient_discharges.discharged_on','newborn_payers.payer_category_id',\n",
    "                 'newborn_payers.plan_code_number','newborns.birth_weight_group_id',\n",
    "                 'newborns.gestational_age_group_id','newborns.plurality_group_id']\n",
    "\n",
    "tableB_fields = ['births.sex_id','births.newborn_id','births.delivery_route_id', \n",
    "            'births.mothers_residence_zip_code','delivery_discharges.race_id','delivery_discharges.ethnicity_id', \n",
    "                'delivery_payers.payer_type_of_coverage_id','delivery_discharges.principal_language_spoken', \n",
    "                 'delivery_discharges.discharged_on','delivery_payers.payer_category_id',\n",
    "                 'delivery_payers.plan_code_number','births.birth_weight',\n",
    "                 'births.gestational_age_ob_estimate','births.plurality'] \n",
    "\n",
    "feature_vals = ['sex_id','c_section','zip_code','race','ethnicity','payer_type_of_coverage_id',\n",
    "                'language_spoken','discharge','payer_category_id','plan_code_number','weight_group_id',\n",
    "                'gest_age_group_id','plurality','weight_plus','weight_minus','gest_age_plus','gest_age_minus']\n",
    "feature_vals = [val for pair in zip([x+'_match' for x in feature_vals], [x+'_nomatch' for x in feature_vals]) for val in pair]\n",
    "\n",
    "# Define bins to use for certain fields\n",
    "age_bins = np.concatenate(([0,1,24], np.arange(25, 36, 2),[99]), axis=0)\n",
    "bw_bins = np.concatenate(([1], np.arange(500, 2001, 250),[2500,9999]), axis=0)\n",
    "\n",
    "##learner_list = [eval(x) for x in classifiers]\n",
    "cesarean_ids = [1, 11, 21, 31, 2, 12, 22, 32]\n",
    "    \n",
    "simple_features = feature_vals[0:26] # Exact matching only\n",
    "complex_features = feature_vals[26:] # Includes non-exact matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def digitize_series(vals,bins):\n",
    "    #digitize_series(cross_table['birth_weight'],bw_bins)\n",
    "    return(pd.Series(np.digitize(list(vals),bins)))\n",
    "\n",
    "def get_condition_vals(cross_table):\n",
    "    condition_vals = [  (cross_table['sex_id_x'],cross_table['sex_id_y']),\n",
    "                (cross_table['cesarean_section'],\n",
    "                 pd.Series([x in cesarean_ids for x in cross_table['delivery_route_id']])),\n",
    "                (cross_table['zip_code'],cross_table['mothers_residence_zip_code']),\n",
    "                (cross_table['race_id_x'],cross_table['race_id_y']),\n",
    "                (cross_table['ethnicity_id_x'],cross_table['ethnicity_id_y']),\n",
    "                (cross_table['payer_type_of_coverage_id_x'],cross_table['payer_type_of_coverage_id_y']),\n",
    "                ((cross_table['principal_language_spoken_x'].str[:3]).str.lower(),\n",
    "                 (cross_table['principal_language_spoken_y'].str[:3]).str.lower()),\n",
    "                (cross_table['discharged_on_x'],cross_table['discharged_on_y']),\n",
    "                (cross_table['payer_category_id_x'],cross_table['payer_category_id_y']),\n",
    "                (cross_table['plan_code_number_x'],cross_table['plan_code_number_y']),\n",
    "                (cross_table['birth_weight_group_id'],\n",
    "                     digitize_series(cross_table['birth_weight'],bw_bins)),\n",
    "                (cross_table['gestational_age_group_id'],\n",
    "                     digitize_series(cross_table['gestational_age_ob_estimate']/7,age_bins)),\n",
    "                ((cross_table['plurality_group_id']==1),(cross_table['plurality']==1)),\n",
    "                (cross_table['birth_weight_group_id'],digitize_series(cross_table['birth_weight'],bw_bins)+1),\n",
    "                (cross_table['birth_weight_group_id'],digitize_series(cross_table['birth_weight'],bw_bins)-1),\n",
    "                (cross_table['gestational_age_group_id'],digitize_series(cross_table['gestational_age_ob_estimate'],age_bins)+1),\n",
    "                (cross_table['gestational_age_group_id'],digitize_series(cross_table['gestational_age_ob_estimate'],age_bins)-1)  ] \n",
    "    return(condition_vals)\n",
    "\n",
    "# STILL NEED TO FIX FUZZY MATCHING- MAKE OUTCOMES FOR SINGLE FIELD MUTUALLY EXCLUSIVE\n",
    "\n",
    "def getBools(cross_table):\n",
    "    def bool_test(condition_val_pair):\n",
    "        condA,condB = condition_val_pair\n",
    "        return(np.vstack([np.array(condA==condB)&condA.notnull()&condB.notnull(),\n",
    "                          np.array((condA!=condB)&condA.notnull()&condB.notnull())]))\n",
    "    condition_vals = get_condition_vals(cross_table)\n",
    "    return(np.concatenate([bool_test(x) for x in condition_vals]).T)\n",
    "\n",
    "def getMissing(cross_table):\n",
    "    def none_test(condition_val_pair):\n",
    "        condA,condB = condition_val_pair\n",
    "        aNones = [x is None for x in condA]\n",
    "        bNones = [x is None for x in condB]\n",
    "        return(np.vstack([np.logical_or(aNones,bNones),np.logical_or(aNones,bNones)]))\n",
    "    condition_vals = get_condition_vals(cross_table)\n",
    "    return(np.concatenate([none_test(x) for x in condition_vals]).T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all possible values for 1st blocking field ('hospital_id')\n",
    "stmt = '''\n",
    "SELECT DISTINCT patient_discharges.hospital_id\n",
    "FROM patient_discharges \n",
    "UNION\n",
    "SELECT DISTINCT births.hospital_id\n",
    "FROM births;'''\n",
    "block_list1 = [x[0] for x in l.exec_sql(stmt).values.tolist()]\n",
    "\n",
    "block_list1 = block_list1[0:2]\n",
    "\n",
    "# Find all possible values for 2nd blocking field ('date_of_delivery')\n",
    "stmt = '''\n",
    "SELECT DISTINCT births.date_of_delivery\n",
    "FROM births;'''\n",
    "block_list2 = [x[0] for x in l.exec_sql(stmt).values.tolist()]\n",
    "\n",
    "block_list2 = block_list2[500:650]\n",
    "\n",
    "block_prod = [(str(x),str(y)) for x in block_list1 for y in block_list2]\n",
    "\n",
    "# Assign strings to select each block\n",
    "blocking_stmt1 = '''\n",
    "        SELECT patient_discharges.id AS pdd_id, newborns.id AS newb_id,%s \n",
    "        FROM %s\n",
    "        WHERE patient_discharges.hospital_id = %r\n",
    "        AND patient_discharges.date_of_birth = %r;\n",
    "        '''\n",
    "blocking_stmt2 = '''\n",
    "        SELECT births.id AS bc_id,%s\n",
    "        FROM %s\n",
    "        WHERE births.hospital_id = %r\n",
    "        AND births.date_of_delivery = %r;\n",
    "        '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create classifiers and iterate over each block\n",
    "- Create [probabilistic] and [perceptron] classifier (train each on selected subset of features ['simple' vs. 'complex'])\n",
    "- Loop over each block and convert to pandas dataframe\n",
    "- Cross join the features-of-interest from both blocks\n",
    "- Apply conditional statements to each column of dataframe\n",
    "- Return match score from each learner\n",
    "- Maximize pairing with Kuhn-Munkres (i.e. Hungarian) Algorithm [more info]\n",
    "- Compare guesses to truth, update weights/probabilities\n",
    "\n",
    "[more info]: https://pypi.python.org/pypi/munkres/\n",
    "[perceptron]: http://glowingpython.blogspot.com/2011/10/perceptron.html\n",
    "[probabilistic]: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5005943/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1, blockA: 26 records, blockB: 26 records  "
     ]
    }
   ],
   "source": [
    "learning_rate_pt = 0.01 # small, constant learning rate for perceptron\n",
    "learning_rate_nn = 0.1\n",
    "\n",
    "# Create instances of classifiers\n",
    "classifiers = [Probabilistic(simple_features),\n",
    "                Probabilistic(feature_vals),\n",
    "                Perceptron(simple_features,learningrate = learning_rate_pt),\n",
    "                Perceptron(feature_vals,learningrate = learning_rate_pt),\n",
    "                NeuralNetwork(feature_vals,10,1,learningrate = learning_rate_nn)]\n",
    "\n",
    "classifier_names = ['pr_simple','pr_complex','pt_simple','pt_complex','nn_complex']\n",
    "\n",
    "#classifier_features = [simple_features,feature_vals,simple_features,feature_vals,feature_vals]\n",
    "class_prog = [True] * len(classifiers)\n",
    "\n",
    "iter_qual_list = []\n",
    "iter_score_list = []\n",
    "block_duration = []\n",
    "epoch_duration = []\n",
    "epoch_count = 0\n",
    "verb_str = '\\rEpoch #{}, blockA: {} records, blockB: {} records  ' # verbose output\n",
    "\n",
    "while True:\n",
    "    epoch_start = time.time() # Measure epoch duration\n",
    "    big_bool = pd.DataFrame(columns=feature_vals)  \n",
    "\n",
    "    for block1, block2 in block_prod:\n",
    "        \n",
    "        block_start = time.time() # Measure block duration\n",
    "\n",
    "        # Create blocks\n",
    "        blockA = l.exec_sql(blocking_stmt1 % (\",\".join(tableA_fields),tableA,block1,block2))\n",
    "        blockB = l.exec_sql(blocking_stmt2 % (\",\".join(tableB_fields),tableB,block1,block2))\n",
    "        \n",
    "        if verbose:\n",
    "            sys.stdout.write(verb_str.format(epoch_count+1, len(blockA), len(blockB)))\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "        # Check that neither block is empty - If empty, skip to next record-pair in loop\n",
    "        if (len(blockA)==0) or (len(blockB)==0):\n",
    "            continue\n",
    "\n",
    "        # Cross-join both blocks\n",
    "        cross_table = l.df_crossjoin(blockA, blockB)\n",
    "\n",
    "        # Count field matches and dump each into new column of dataframe\n",
    "        bool_table = pd.DataFrame.from_items(zip(feature_vals,getBools(cross_table).T))\n",
    "        # bool_table = pd.DataFrame.from_items(zip(feature_vals,[eval(x).values for x in conditions]))\n",
    "\n",
    "        # Add record-id columns to boolean table\n",
    "        bool_table['newb_id'] = cross_table['newb_id'].values # Actual newborn id\n",
    "        bool_table['bc_id'] = cross_table['bc_id'].values  # Actual bc id\n",
    "        bool_table['prev_newb_id'] = cross_table['newborn_id'].values # Originally matched newborn id \n",
    "        \n",
    "        # Compare with previously-linked newborn id\n",
    "        bool_table['real_match'] = (cross_table['newborn_id']==cross_table['newb_id']).values \n",
    "        for x in classifier_names:\n",
    "            bool_table['match_'+x] = False # Create new columns for later\n",
    "        \n",
    "        # STILL NEED TO RE-DISTRIBUTE THE WEIGHTS FOR MISSING VALUES\n",
    "        # Get guesses from classifiers\n",
    "        record_id = bool_table[['newb_id','bc_id']]\n",
    "        for x,y in zip(classifiers,classifier_names):\n",
    "            link_inds,link_score = x.query(bool_table[x.features],record_id) \n",
    "            bool_table['lscore_'+y] = link_score\n",
    "            bool_table.loc[link_inds,'match_'+y] = True # Label nominees as such  \n",
    "        \n",
    "        # Aggregate field-match tables (Booleans) within loop\n",
    "        big_bool = pd.concat([big_bool,bool_table])\n",
    "    \n",
    "        # Time duration of each block\n",
    "        block_duration.append(time.time()-block_start)\n",
    "        \n",
    "    # Check progress of each classifier \n",
    "    classifiers_masked = [elem for elem,z in zip(classifiers,class_prog) if z==True]\n",
    "    classifier_names_masked = [elem for elem,z in zip(classifier_names,class_prog) if z==True]\n",
    "    \n",
    "    # Train classifiers (if still progressing)\n",
    "    train_classifier = lambda x,y: x.train(big_bool[x.features],big_bool['real_match'],big_bool['match_'+y])\n",
    "    map(train_classifier,classifiers,classifier_names)\n",
    "    \n",
    "    # Measure accuracy of each classifier\n",
    "    score_list = [((big_bool['match_'+x]==big_bool['real_match'])*1).tolist() for x in classifier_names]\n",
    "    cur_scores = [sum(x)*100.0/len(x) for x in score_list]\n",
    "    iter_score_list.append(cur_scores)\n",
    "    \n",
    "    # Set range of linkage scores (0 through 100 percentiles) i.e. decision threshold (theta)\n",
    "    set_thetas = range(0,100,10)\n",
    "    \n",
    "    # Update precision & recall of classifiers\n",
    "    for x,y in zip(classifiers,classifier_names):\n",
    "        x.prec_recall(big_bool['lscore_'+y],set_thetas,big_bool['real_match'])\n",
    "    \n",
    "    # Compare current iteration 'quality' with previous iterations\n",
    "    cur_iter_qual = [x.iter_qual_list[-1] for x in classifiers]\n",
    "    prev_iter_qual = [x.iter_qual_list[-2] for x in classifiers]\n",
    "    class_prog = np.subtract(cur_iter_qual,prev_iter_qual)>.1\n",
    "    iter_qual_list.append(cur_iter_qual)\n",
    "    \n",
    "    # If not improving > Break from outer loop\n",
    "    if not any(class_prog):\n",
    "        print str.format('\\n'+'-'*80+\"\\n'Quality' achieved! @ {}\",[ float('%.2f' % x) for x in cur_iter_qual ])        \n",
    "        break\n",
    "\n",
    "    # Time duration of each iteration\n",
    "    epoch_duration.append(time.time()-epoch_start)\n",
    "    \n",
    "    epoch_count += 1 # Increment epoch counter\n",
    "    \n",
    "    if verbose:\n",
    "        print str.format('\\nAverage block duration: {0:.2f} seconds', np.mean(block_duration))\n",
    "        print str.format(\"'Quality': {}\", [ float('%.2f' % x) for x in cur_iter_qual ])\n",
    "        print str.format('% correct: {}', [ float('%.4f' % x) for x in cur_scores ])\n",
    "        print str.format('Average epoch duration: {0:.3f} minutes', np.mean(epoch_duration)/60.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot precision/recall for each epoch of probabilistic record linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prec_list = [o.precision_list for o in classifiers]\n",
    "plot_recall_list = [o.recall_list for o in classifiers]\n",
    "color_list = ['b','g','r','c','m'][slice(len(classifiers))]\n",
    "\n",
    "ncol = int(np.ceil(np.sqrt(len(plot_prec_list[0]))))\n",
    "nrow = int(np.ceil(len(plot_recall_list[0])/float(ncol)))\n",
    "fig, axs = plt.subplots(nrow, ncol, sharex='col', sharey='row')\n",
    "for i, ax in enumerate(fig.axes[0:len(plot_prec_list[0])]):   \n",
    "    plot_curves = lambda w,x,y: ax.plot(w[i], x[i], '-', linewidth=2, marker='o', color=y)\n",
    "    map(plot_curves,plot_prec_list,plot_recall_list,color_list)\n",
    "    ax.set_xlim([0,110])\n",
    "    ax.set_ylim([0,110])\n",
    "    ax.set_title(\"iter. # {}\".format(str(i)), fontsize=12)\n",
    "    ax.set(aspect='equal')    \n",
    "else:\n",
    "    ax.set_xlabel('precision', fontsize=12)\n",
    "    ax.set_ylabel('recall', fontsize=12)\n",
    "\n",
    "fig.set_size_inches(4.875*ncol,4.5*nrow)\n",
    "plt.show()\n",
    "fig.savefig('prec_recall.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot accuracy for each classifier over all epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(1,epoch_count+2)\n",
    "y = 100-np.array(iter_score_list, ndmin=2, dtype=np.float).T\n",
    "fig = plt.figure()\n",
    "plt.yscale('log')\n",
    "plt.ylim((y.min(),100))\n",
    "plt.title(\"classifier accuracy\")\n",
    "for quals,cl,col in zip(y,classifier_names,color_list):\n",
    "    plt.plot(x,quals,label = cl+' [min:'+str('%.2f' % quals.min())+'%]',color=col)\n",
    "plt.xlabel(\"epoch count\")\n",
    "plt.ylabel(\"% incorrect\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('classifier_accuracy.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show table of weights/probabilities for each classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assemble table elements into dataframe\n",
    "pt_simple_weights = classifiers[2].weights.tolist()[:-1] + \\\n",
    "    [None]*len(complex_features) + [classifiers[2].weights.tolist()[-1]]\n",
    "\n",
    "weight_list = [['prob_simple'] + classifiers[0].m_probs.tolist() + [None],\n",
    "               ['prob_complex'] + classifiers[1].m_probs.tolist() + [None],\n",
    "               ['perc_simple'] + pt_simple_weights,\n",
    "               ['perc_complex'] + classifiers[3].weights.tolist()]\n",
    "df = pd.DataFrame(weight_list, columns=['classifier']+feature_vals+['bias'])\n",
    "\n",
    "# Save table as html file\n",
    "my_file = open('weights.html', 'w')\n",
    "my_file.write(df.to_html())\n",
    "#pt_simple_weights = classifiers[2].weights.tolist()\n",
    "my_file.close()\n",
    "\n",
    "# Show table as ipython figure\n",
    "itable.PrettyTable(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organize and export 'wrong answers' to spreadsheet (.csv) files [1 spreadsheet/classifier]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:11: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Select features to export\n",
    "feature_list = ['bc_id','real_match','newb_id']+feature_vals\n",
    "\n",
    "# Gather incorrect matches for each classifier\n",
    "score_list = [((big_bool['match_'+x]==big_bool['real_match'])*1).tolist() for x in classifiers]\n",
    "\n",
    "for sc,cl in zip(score_list,classifiers):\n",
    "    errors = big_bool[feature_list].iloc[[x==0 for x in sc],:].sort(['bc_id','real_match'], ascending=[1,0])\n",
    "    errors.to_csv('linkage_error_'+cl+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n"
     ]
    }
   ],
   "source": [
    "nn_complex = NeuralNetwork(feature_vals,10,1,learningrate = learning_rate_nn) # multi-layer neural network\n",
    "\n",
    "inputs = np.array(bool_table[feature_vals], ndmin=2, dtype=np.float).T\n",
    "\n",
    "# calculate signals into hidden layer\n",
    "hidden_inputs = np.dot(nn_complex.wih, inputs)\n",
    "# calculate the signals emerging from hidden layer\n",
    "hidden_outputs = nn_complex.activation_function(hidden_inputs)\n",
    "\n",
    "# calculate signals into final output layer\n",
    "final_inputs = np.dot(nn_complex.who, hidden_outputs)\n",
    "# calculate the signals emerging from final output layer\n",
    "final_outputs = nn_complex.activation_function(final_inputs)\n",
    "\n",
    "#if nn_complex.onodes==2:\n",
    "#    final_score = np.sqrt(final_outputs[1]*(1-final_outputs[0]))\n",
    "\n",
    "# Convert to linkage score to linkage 'cost'\n",
    "score_cost = abs(final_outputs - final_outputs.max())\n",
    "\n",
    "# Maximize pair-wise linkage scores (minimize cost with Munkres)\n",
    "link_list = bool_table[['newb_id','bc_id']].assign(linkage_cost = score_cost.T)\n",
    "winner_ind = nn_complex.maximize(link_list)\n",
    "\n",
    "# Return boolean array\n",
    "winners = np.zeros(len(link_list), np.bool)\n",
    "winners[winner_ind] = 1\n",
    "\n",
    "# Convert to binary output\n",
    "class_outputs = final_outputs>.5\n",
    "\n",
    "print (hidden_inputs)\n",
    "#print(winners.T,final_outputs.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-09479593cae2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwho\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwih\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mwho\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwih\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mwho_update_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mwih_update_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-09479593cae2>\u001b[0m in \u001b[0;36mreset_weights\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     who = np.array([[0.1, 0.1, 0.1],\n\u001b[0m\u001b[1;32m      5\u001b[0m        [ 0.1,  0.1, 0.1]], ndmin=2, dtype=np.float)\n\u001b[1;32m      6\u001b[0m     wih = np.array([[0.1, 0.1, 0.1, 0.1],\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "#nn_complex = NeuralNetwork(len(conditions[:4]),3,2,learning_rate) # multi-layer neural network\n",
    "\n",
    "def reset_weights():\n",
    "    who = np.array([[0.1, 0.1, 0.1],\n",
    "       [ 0.1,  0.1, 0.1]], ndmin=2, dtype=np.float)\n",
    "    wih = np.array([[0.1, 0.1, 0.1, 0.1],\n",
    "       [ 0.1,  0.1, 0.1, 0.1],\n",
    "       [ 0.1,  0.1, 0.1, 0.1]], ndmin=2, dtype=np.float)\n",
    "    return who,wih\n",
    "\n",
    "who,wih = reset_weights()\n",
    "who_update_list = []\n",
    "wih_update_list = []\n",
    "\n",
    "def test_training(rows,who_update_list,wih_update_list):\n",
    "    inputs_list = big_bool[feature_vals[:4]].iloc[rows]*.98+.01 # scale inputs to between .01 and .99\n",
    "    truth_list = big_bool['real_match'].iloc[rows]*.98+.01\n",
    "\n",
    "    if not isinstance(truth_list, pd.Series):\n",
    "        truth_list = [truth_list]\n",
    "    \n",
    "    print('truth_list',truth_list)\n",
    "    \n",
    "    # convert inputs list to 2d array\n",
    "    inputs = np.array(inputs_list.values.tolist(), ndmin=2, dtype=np.float).T\n",
    "    truths = np.array([ [.99,.01] if x<.5 else [.01,.99] for x in truth_list ], ndmin=2, dtype=np.float).T\n",
    "    print('truths',truths)\n",
    "    # calculate signals into hidden layer\n",
    "    hidden_inputs = np.dot(wih, inputs)\n",
    "    # calculate the signals emerging from hidden layer\n",
    "    hidden_outputs = nn_complex.activation_function(hidden_inputs)\n",
    "\n",
    "    # calculate signals into final output layer\n",
    "    final_inputs = np.dot(who, hidden_outputs)\n",
    "    # calculate the signals emerging from final output layer\n",
    "    final_outputs = nn_complex.activation_function(final_inputs)\n",
    "\n",
    "    # output layer error is the (truth - guesses)\n",
    "    output_errors = truths - final_outputs\n",
    "\n",
    "    # hidden layer error is the output_errors, split by weights, recombined at hidden nodes\n",
    "    hidden_errors = np.dot(who.T, output_errors) \n",
    "\n",
    "    # update the weights for the links between the hidden and output layers\n",
    "    who_update = nn_complex.lr * np.dot((output_errors * final_outputs * (1.0 - final_outputs)), np.transpose(hidden_outputs))\n",
    "\n",
    "    # update the weights for the links between the input and hidden layers\n",
    "    wih_update = nn_complex.lr * np.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), np.transpose(inputs))\n",
    "\n",
    "    who_update_list.append(who_update)\n",
    "    wih_update_list.append(wih_update)\n",
    "    \n",
    "    return who_update,wih_update,who_update_list,wih_update_list\n",
    "\n",
    "test_len = 3\n",
    "for i in range(test_len):\n",
    "    rows = slice(i,i+1)\n",
    "\n",
    "    who_update,wih_update,who_update_list,wih_update_list = test_training(rows,who_update_list,wih_update_list)\n",
    "    who += who_update\n",
    "    wih += wih_update\n",
    "\n",
    "#print('who_update',who_update)\n",
    "#print('wih_update',wih_update)\n",
    "#print('who_update_list',who_update_list)\n",
    "#print('wih_update_list',wih_update_list)\n",
    "\n",
    "who,wih = reset_weights()\n",
    "who_mat_update,wih_mat_update,_,_ = test_training(slice(0,test_len),[],[])\n",
    "print('who_update_list sum',np.sum(who_update_list,0))\n",
    "print('who_mat_update',who_mat_update)\n",
    "print('wih_update_list sum',np.sum(wih_update_list,0))\n",
    "print('wih_mat_update',wih_mat_update)\n",
    "\n",
    "#print('wih_mat_update',wih_mat_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0. -1.  0.  0. -1.  0. -1. -1. -1. -1. -1. -1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# Show equivalence between row-iteration and matrix multiplication\n",
    "inputs_list = bool_table[feature_vals]\n",
    "predictions,score = pt_complex.query(inputs_list)\n",
    "targets = np.array(bool_table['match'], ndmin=2)\n",
    "\n",
    "inputs = np.array(inputs_list, ndmin=2)\n",
    "inputs = np.concatenate((inputs,np.repeat(1, len(inputs[:,1]))[:, None]), axis=1) # Add bias input\n",
    "weights = np.random.uniform(low=-0.01, high=0.01, size=len(inputs[1,:]))\n",
    "    \n",
    "output_errors = targets-predictions\n",
    "print output_errors\n",
    "#print inputs.shape,weights.shape,output_errors.shape\n",
    "\n",
    "#print learning_rate * np.dot(output_errors, inputs)\n",
    "\n",
    "for i,row in enumerate(inputs):\n",
    "    foo = output_errors[:,i] * row\n",
    "    weights += learning_rate * output_errors[:,i] * row\n",
    "#print weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "32\n",
      "484\n",
      "84.2105263158 4.59110473458\n"
     ]
    }
   ],
   "source": [
    "cutoff = theta[-1]\n",
    "winners = (big_bool['linkage_score']>cutoff)\n",
    "print sum(winners&big_bool['real_match'])\n",
    "print sum(winners&big_bool['pair_match']&big_bool['real_match'])\n",
    "print sum(big_bool['pair_match']&big_bool['real_match'])\n",
    "\n",
    "# Precision: the percent of pairs with a score above theta that are real matches\n",
    "# USE ONLY NOMINATED PAIRS\n",
    "# - numerator: number of NOMINATED pairs with a score above theta that are real matches\n",
    "real_picks = sum(winners&big_bool['real_match']&big_bool['pair_match'])\n",
    "# - denominater: number of NOMINATED pairs with a score above theta\n",
    "picks = sum(winners&big_bool['pair_match'])\n",
    "precision = (real_picks/float(picks)*100)\n",
    "\n",
    "# Recall: the percent of known matched pairs that get a score above theta\n",
    "# - numerator: number of NOMINATED pairs with a score above theta that are real matches\n",
    "# - denominater: number of pairs that are real matches\n",
    "real_matches = big_bool['real_match'].sum()\n",
    "recall = (real_picks/float(real_matches)*100)\n",
    "\n",
    "print precision,recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "[1,  '< 500g',          1,    1 ..  499, 1],\n",
    "    [2,  '500 to 749g',     2,  500 ..  749, 1],\n",
    "    [3,  '750 to 999g',     3,  750 ..  999, 1],\n",
    "    [4,  '1,000 to 1,250g', 4, 1000 .. 1250, 2],\n",
    "    [5,  '1,250 to 1,499g', 5, 1250 .. 1499, 2],\n",
    "    [6,  '1,500 to 1,749g', 6, 1500 .. 1749, 3],\n",
    "    [7,  '1,750 to 1,999g', 7, 1750 .. 1999, 4],\n",
    "    [8,  '2,000 to 2,499g', 8, 2000 .. 2499, 5],\n",
    "    [9,  '> 2,500g',        9, 2500 .. 9999, 6],\n",
    "    # Overweight?!?\n",
    "    [11, 'Unknown',         0, []],\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "data = list(cross_table['birth_weight'])\n",
    "        \n",
    "bw_bins = np.concatenate(([1], np.arange(500, 2001, 250),[2500,9999]), axis=0)\n",
    "print bw_bins\n",
    "\n",
    "inds = np.digitize(data, bw_bins)\n",
    "\n",
    "for n in range(len(data)):\n",
    "    print(bw_bins[inds[n]-1], \"<=\", data[n], \"<\", bw_bins[inds[n]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = cross_table['gestational_age_ob_estimate']/7\n",
    "\n",
    "# '<24 weeks','24 weeks','25-26 weeks','27-28 weeks',...'35-36 weeks','37 or more weeks'\n",
    "        \n",
    "bins = np.concatenate(([0,24], np.arange(25, 36, 2),[99]), axis=0)\n",
    "print bins\n",
    "\n",
    "inds = np.digitize(data.tolist(), bins)\n",
    "\n",
    "for n in range(len(data)):\n",
    "    print(bins[inds[n]-1], \"<=\", data[n], \"<\", bins[inds[n]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Find relative frequencies of values in each field\\n# - save each as dictionary (1 dict per field per table)\\nrel_freq_tA = {}\\nfor i in tableA_fields:\\n    stmt = '''SELECT {0} value, COUNT(*) count\\n    FROM {1}\\n    GROUP BY {0};'''.format(i,tableA)\\n    field_dict = exec_statement(stmt,'dict')\\n    rel_freq_tA[i] = dict(zip(field_dict['value'],field_dict['count']))\\n    \\nrel_freq_tB = {}\\nfor i in tableB_fields:\\n    stmt = '''SELECT {0} value, COUNT(*) count\\n    FROM {1}\\n    GROUP BY {0};'''.format(i,tableB)\\n    field_dict = exec_statement(stmt,'dict')\\n    rel_freq_tB[i] = dict(zip(field_dict['value'],field_dict['count']))\\n\\n# BROKEN\\ndef rel_freq(table_fields,table):\\n    # Find relative frequencies of values in each field\\n    # - save each as dictionary (1 dict per field per table)\\n    freq_out = {}\\n    for i in table_fields:\\n        stmt = '''SELECT {0} value, COUNT(*) count\\n        FROM {1}\\n        GROUP BY {0};'''.format(i,table)\\n        field_dict = l.exec_sql(stmt).to_dict()\\n        print field_dict\\n        freq_out[i] = dict(zip(field_dict['value'],field_dict['count']))\\n    \\nrel_freq_tA = rel_freq(tableA_fields,tableA)\\nrel_freq_tB = rel_freq(tableB_fields,tableB)\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Find relative frequencies of values in each field\n",
    "# - save each as dictionary (1 dict per field per table)\n",
    "rel_freq_tA = {}\n",
    "for i in tableA_fields:\n",
    "    stmt = '''SELECT {0} value, COUNT(*) count\n",
    "    FROM {1}\n",
    "    GROUP BY {0};'''.format(i,tableA)\n",
    "    field_dict = exec_statement(stmt,'dict')\n",
    "    rel_freq_tA[i] = dict(zip(field_dict['value'],field_dict['count']))\n",
    "    \n",
    "rel_freq_tB = {}\n",
    "for i in tableB_fields:\n",
    "    stmt = '''SELECT {0} value, COUNT(*) count\n",
    "    FROM {1}\n",
    "    GROUP BY {0};'''.format(i,tableB)\n",
    "    field_dict = exec_statement(stmt,'dict')\n",
    "    rel_freq_tB[i] = dict(zip(field_dict['value'],field_dict['count']))\n",
    "\n",
    "# BROKEN\n",
    "def rel_freq(table_fields,table):\n",
    "    # Find relative frequencies of values in each field\n",
    "    # - save each as dictionary (1 dict per field per table)\n",
    "    freq_out = {}\n",
    "    for i in table_fields:\n",
    "        stmt = '''SELECT {0} value, COUNT(*) count\n",
    "        FROM {1}\n",
    "        GROUP BY {0};'''.format(i,table)\n",
    "        field_dict = l.exec_sql(stmt).to_dict()\n",
    "        print field_dict\n",
    "        freq_out[i] = dict(zip(field_dict['value'],field_dict['count']))\n",
    "    \n",
    "rel_freq_tA = rel_freq(tableA_fields,tableA)\n",
    "rel_freq_tB = rel_freq(tableB_fields,tableB)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update 'births' table accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "foo = fin_bool.loc[big_bool['pair_match']==1]\n",
    "for index, row in foo.iterrows():     \n",
    "    stmt1 = '''\n",
    "        UPDATE births \n",
    "        SET newborn_id_2 = %d, newborn_linkage_score_2 = %s\n",
    "        WHERE id = %d;\n",
    "        ''' % (row['newb_id'], row['linkage_score'], row['bc_id'])\n",
    "    exec_statement(stmt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
